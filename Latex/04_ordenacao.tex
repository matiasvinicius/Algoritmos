\documentclass[a4paper, twocolumn]{article}

\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{a4wide}
\usepackage{float}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{enumerate}
\usepackage{url}
\usepackage{listings}
\usepackage{caption}

\lstset{
	language=Python,
	basicstyle=\small\sffamily,
	numbers=left,
	numberstyle=\tiny,
	xleftmargin = 3em,
	frame=tb,
	columns=fullflexible,
	showstringspaces=false,
}

\graphicspath{ {images/} }
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\title{Algoritmos de ordenação}
\author{Vinicius A. Matias}
\date{\today}
\begin{document}
	\maketitle

	\section{Introdução}
	Ordnear elementos de uma estrutura de dados é uma das tarefas mais curiosas e da computação. Diferentes implementações podem ser realizadas, das mais triviais às mais eficientes, assumindo heurísticas dos algoritmos ou não, e cada um dos algoritmos tem seu valor para um melhor entendimento da complexidade computacional.
	
	\section{Insertion Sort}
	O método de ordenação por inserção é um dos maios simples. Ele consiste em verificar de dois em dois elementos se o valor corrente é maior que o valor antecessor no arranjo, e se for, trocar e repetir o processo até o arranjo ficar ordenado.
	
	Esse algoritmo pode ser pensado no caso de segurarmos um conjunto de cartas em uma mão e queiramos ordená-las. Aplicando o algoritmo da ordenação por inserção, verificamos a segunda carta, vemos se a primeira é maior que ela e se sim, trocamos as cartas. Na terceira carta vemos se a segunda é maior, e se for, vericamos se ela também é maior que a primeira. Esse processo é repetido para todas as cartas, passando por cada carta da esquerda para a direita e comparando da direita para a esquerda.
	
	\subsection{Implementação iterativa}	
	
	A implementação do algoritmo iterativo pode ser vista na Listing 1.
		
	Como a operação de interesse desse algoritmo é a comparação entre elementos do arranjo e o valor corrente, devemos identificar que existem dois laços envolvidos para computar essa operação. O laço mais externo roda entre $i=0$ e $i<len(A)$, ou seja, $n-1$ vezes. No melhor dos casos não haverá necessidade de trocar os elementos pois o arranjo já está ordenado, e nesse caso só será realizada uma verificação por volta do laço, levando à $n-1$ iterações no melhor caso. 

\begin{lstlisting}[label=insertion_sort,caption= Insertion Sort iterativo]
def insertion_sort(A):
    end = len(A)
    i = 0
    j = 0

    while i < end:
        value = A[i]
        j = i
        while j>0 and A[j-1] > value:
            A[j] = A[j-1]
            j = j-1
        A[j] = value
        i += 1
    
    return A
\end{lstlisting}	
	
	
	No pior caso, para cada uma das $n-1$ voltas do laço será necessário verificar todos os elementos anteriores à posição atual (segundo laço). Isso implica que a primeira execução fará uma verificação, a segunda duas, a terceira três até a enésima, realizando n comparações. Somando o número de comparações teremos algo como $1+2+3+...+n-1$, que pode ser vista como uma soma de Progressão Aritmética. A soma desta PA que cresce de 1 em um pode ser definida como $\frac{n(0+n-1)}{2} = \frac{n^2-n}{2}$, isto é, o algoritmo de ordenação por inserção iterativo tem crescimento assintótico $\mathcal{O}(n^2)$
	
\subsection{Demonstração de crescimento assintótico $\mathcal{O}(n^2)$}
	
$f(n) = \frac{n^2-n}{2} \in \mathcal{O}(n^2)$ se existem constantes $n_0 \geq 0$ e $c \geq 0$ que satisfazem a inequação:

$0 \leq \frac{n^2-n}{2} \leq cn^2$ \\
 
Resolvendo $0 \leq \frac{n^2-n}{2}$ notamos que a inequação é verdadeira para qualquer $n \in \mathcal{R}$ \\

Resolvendo $\frac{n^2-n}{2} \leq cn^2$:

$\frac{n(n-1)}{2} \leq cn^2$

$\frac{n-1}{2} \leq cn$

$\frac{-1}{2} \leq cn - \frac{n}{2}$

$\frac{-1}{2} \leq n(c - \frac{1}{2})$

Para a inequação ser verdadeira, $c - \frac{1}{2} \geq 0$ deve ser verdade, logo, $c \geq \frac{1}{2}$.

Assim, a inequação $\frac{-1}{2} \leq n(c - \frac{1}{2})$ é verdadeira para qualquer valor $n \geq 0$ e $c \geq \frac{1}{2}$, como exemplo:

$n_0 = 1$ e $c = 1$ \\

Isso prova que $\frac{n^2-n}{2} \in \mathcal{O}(n^2)$

\subsection{Implementação recursiva}	

A listing 2 apresenta uma implementação recursiva do Insertion Sort.

O algoritmo é baseado na indução fraca, garantindo que sabe-se ordenar um arranjo com um elemento, pois ele já está ordenado (caso base). Cada chamada recursiva um dos $n-1$ sub arranjos possíveis.

\begin{lstlisting}[label=insertion_sort_rec,caption= Insertion Sort recursivo]
def insertion_sort_rec(A, n):
    if n == 1: return 

    insertion_sort_rec(A, n-1)
    i = n - 1
    aux = 0

    while i > 0 and A[i-1] > A[i]:
        aux = A[i]
        A[i] = A[i-1]
        A[i-1] = aux
        i -= 1

    return A
\end{lstlisting}
	
Considerando que a operação de interesse é a comparação entre elementos do arranjo $A[i-1] > A[i]$, podemos definir duas equações de recorrência, uma para o melhor caso e outra para o pior caso.

\textbf{Melhor caso: } O arranjo está ordenado, portanto serão feitas $n-1$ chamadas recursivas para um arranjo de tamanho $n$, e uma comparação em cada uma dessas chamadas. Para o caso base não é feita nenhuma comparação entre elementos do arranjo.

$T(n) = \begin{cases} 
	0, \ n  = 1\\
	T(n-1) + 1, \ n > 1
\end{cases}$ \\

A resolução dessa equação de recorrência diz que $T(n) = n - 1$ \\

\textbf{Pior caso: } São feitas $n-1$ comparação para cada $n$ passado na recorrência.

$T(n) = \begin{cases} 
	0, \ n  = 1\\
	T(n-1) + n-1, \ n > 1
\end{cases}$ \\

E a resolução dessa equação de recorrência resulta em $T(n) = (n^2-n)/2$, e que $T(n) \in \mathcal{O}(n^2)$

\subsection{Equação de recorrência para o pior caso}

Demonstraremos que o resultado da equação de recorrência abaixo é $T(n) \in \mathcal{O}(n^2)$ \\

$T(n) = \begin{cases} 
	0, \ n  = 1\\
	T(n-1) + n-1, \ n > 1
\end{cases}$ \\

Notando que $T(n)$ varia as chamadas recursivas de 1 em 1, calcularemos as equações de recorrência para $n-1$, $n-2$ e $n-3$

$T(n) = T(n-1) + n-1$\\

$T(n-1) = T(n-1-1) + n-1-1 = T(n-2) + n - 2$\\

$T(n-2) = T(n-2-1) + n-2-1 = T(n-3) + n - 3$\\

$T(n-3) = T(n-3-1) + n-3-1 = T(n-4) + n - 4$\\

Podemos aplicar esses valores na equação de recorrência $T(n)$:

$T(n) = T(n-1) + n-1$

$T(n) = T(n-2) + n - 2 + n-1$

$T(n) = T(n-2) +2n -2 -1$

$T(n) = T(n-3) + n - 3 +2n -2 -1$

$T(n) = T(n-3) + 3n -3 -2 -1$

$T(n) = T(n-4) + n - 4 + 3n -3 -2 -1$

$T(n) = T(n-4) + 4n -4 -3 -2 -1$

[...]

$T(n) = T(n-i) + in + \sum_{j=1}^{i} -j$

A operação $\sum_{j=1}^{i} -j$ é uma soma de Progressão Aritmética, podendo ser reescrita como $\frac{i*(-1 - i)}{2}$

Assim, $T(n) = T(n-i) + in + \frac{i*(-1 - i)}{2}$

Quando $i = n$: 

$T(n) = T(n-i) + in + \frac{i*(-1 - i)}{2}$

$T(n) = T(n-n) + n^2 + \frac{n*(-1 - n)}{2}$

$T(n) = T(0) + n^2 + \frac{(-n - n^2)}{2}$

$T(n) = n^2 + \frac{(-n - n^2)}{2}$

$T(n) = \frac{(-n - n^2 + 2n^2)}{2}$

$T(n) = \frac{(n^2 - n)}{2}$\\

E como foi demonstrado no item \textbf{2.2}, $\frac{(n^2 - n)}{2} \in \mathcal{O}(n^2)$, portanto: \\

$T(n) \in \mathcal{O}(n^2)$

\section{Selection Sort}
A ordenação por seleção parte do último elemento de um arranjo e compara com todos os anteriores para verificar se há um elemento maior que ele e, caso exista, capturar o maior de todo o subarranjo. O índice do maior valor do subarranjo é capturado e é comparado com o índice elemento que se partiu a ordenação (da direita para a esquerda, então, o último, penúltimo etc) para verificar se são iguais, caso forem iguais não há necessidade de trocar de posições pois ho maior elemento do subarranjo já está mais à direita do arranjo. Caso sejam diferentes, o algoritmo troca a posição do então último elemento do arranjo pelo maior elemento encontrado.

\subsection{Implementação iterativa}

Uma implementação em Python do algoritmo de seleção pode ser vista na Listing 3.

\begin{lstlisting}[label=selection_sort,caption= Selection Sort iterativo]
def selection_sort(A):
    n = len(A)
    fim = n-1

    while fim > 0:
        max = fim
        for j in range(fim):
            if A[j] > A[max]:
                max = j
        if fim != max:
            temp = A[fim]
            A[fim] = A[max]
            A[max] = temp
        fim -= 1

    return A
\end{lstlisting}

A operação de interesse aqui é a comparação entre cada elemento de um subarranjo com o máximo encontrado até então. Note que tanto o loop mais externo quanto o mais interno (que compreende a comparação $A[j] > A[max]$) são executados sempre a mesma quantidade de vezes para um mesmo $n$, implicando que o melhor e o pior caso sejam iguais.

O loop externo é executado $n-1$ vezes e a quantidade de iterações do loop interno segue um progressão aritmética ($n-1$, $n-2$, ..., $2$, $1$). A soma dessas iterações é dada por $\frac{(n-1)*(n-1+1)}{2}$, logo, $\frac{n^2-n}{2}$ comparações para qualquer caso. Isso implica que esta implementação $\in \Theta(n^2)$

\subsection{Implementação recursiva}

Uma implementação recursiva do algoritmo de ordenação por seleção é exibido na Listing 4.

\begin{lstlisting}[label=selection_sort_rec,caption= Selection Sort recursivo]
def selection_sort_rec(A, n):
    if n == 1: return A
    
    max = n-1
    for i in range(n):
        if A[i] > A[max]:
            max = i
    
    if max != n-1:
        temp = A[max]
        A[max] = A[n-1]
        A[n-1] = temp

    return selection_sort_rec(A, n-1)
\end{lstlisting}

Partindo do caso base, sabe-se ordenar um arranjo de apenas um elemento (é o próprio arranjo). Para se ordenar para mais um elemento devem ser seguidas as diretrizes do algoritmo, como foi exibido acima. A equação de recorrência para esse ordenador (para o número de comparações entre elementos do arranjo) pode ser definida como: \\

$T(n) = \begin{cases} 
	0, \ n = 1\\
	T(n-1) + n-1, \ n > 1
\end{cases}$ \\

Levando à $T(n) = \frac{n^2-n}{2}$ e, tanto no melhor quanto no pior caso, à uma implementação $\in \Theta(n^2)$.

\section{Bubble Sort}

O Bubble Sort é possivelmente o método de ordenação mais simples dos aqui estudados, contudo também o método que tem pior desempenho em aplicações reais. O algoritmo consiste em passar por todos os possíveis pares de elementos e comparar se um é maior que o outro. 

\subsection{Implementação iterativa}

O método bolha consiste apenas em uma troca de elementos em pares, uma das implementações iterativas possíveis está na Listing 5.

\begin{lstlisting}[label=bubble_sort,caption= Bubble Sort iterativo]
def bubble_sort(A, n):
    i = n-1
    
    while i > 0:
        j = 1
        while j <= i:
            if A[j-1] > A[j]:
                temp = A[j-1]
                A[j-1] = A[j]
                A[j] = temp
            j += 1
        i -= 1
    
    return A
\end{lstlisting}

Sendo a comparação de interesse destacada em $A[j-1] > A[j]$, assim como no selection sort essa operação será realizada $n-1, n-2, ..., 2, 1$ vezes, mudando agora quais pares de elementos são comparados. A complexidade assintótica se mantém como $\Theta(n^2)$, pois a quantidade de operações segue a mesma soma de PA que resulta em $\frac{(n^2-n)}{2}$.
	
\subsection{Implementação recursiva}

\begin{lstlisting}[label=bubble_sort_rec,caption= Bubble Sort recursivo]
def bubble_sort_rec(A, n):
    i = 1
    while i < n:
        if A[i] < A[i-1]:
            temp = A[i]
            A[i] = A[i-1]
            A[i-1] = temp
        i += 1
    return bubble_sort(A, n-1)
\end{lstlisting}

Ainda que não seja comum, o bubble sort pode ser implementado recursivamente, trocando o laço mais externo por um chamada à própria função para $n-1$. A equação de recorrência é a mesma do selection sort recursivo, levando à uma complexidade $\mathcal{O}(n^2)$.


\section{Merge Sort}
O Merge Sort é um método de ordenação que utliza a técnica de Divisão e Conquista também chamado de ordenação por intercalação. Ele parte da hipótese de indução forte de que sabemos ordenar um conjunto de elementos no intevalo $[1,n]$. A ordenação é feita dividindo o arranjo no meio para gerar dois subarranjos (esquerdo e direito). Para cada subarranjo será feito o mesmo processo de dividir entre esquerdo e direito até uma chamada para um arranjo de apenas um elemento, neste caso armazenaremos o valor e poderemos partir para a chamada do lado direito. Após algumas chamadas recursivas, teremos um elemento do arranjo esquerdo e um elemento do arranjo direito. Estes dois serão passados para a função merge, que será responsável por comparar os dois elementos e ordená-los. 

Esse processo cresce conforme a pilha é desempilhada, havendo comparações dos elementos do arranjo esquerdo com o direito até um limite de $n/2$ comparações, que é o caso da primeira chamada recursiva realizada. O Merge Sort permite que ordenemos arranjos "da esquerda" e "da direita", e isso permite que na próxima comparação, os dois subarranjos passados para o merge estejam ordenados, cabendo à cada chamada do merge comparar o novo esquerdo e direito e ordená-los. 

O Merge Sort normalmente é implementado em duas funções: uma para a ordenação de dois arranjos diferentes e gerar um único ($merge()$) e uma para realizar as chamadas recursivas para dividir os arranjos ($merge\_sort()$).

\begin{lstlisting}[label=merge_sort,caption= Merge sort]
def merge_sort(A):
    if len(A) > 1:
        meio = len(A) // 2
        esq = merge_sort(A[:meio])
        dir = merge_sort(A[meio:])
        A = merge(esq, dir)
    return A
\end{lstlisting}

Diferentes implemementações podem ser obtidas para retornar o mesmo resultado. Muitos materias utilizam variáveis de inicio e fim para gerir o meio do arranjo, e sendo passadas nas chamadas recursivas. Dependendo das estruturas que a linguagem fornece, a implementação pode ser mais sucinta, como é o caso de Python. O pseudocódigo do livro Algoritmos (Cormen et al.), já mencionados em outro relatório fornece um pseudocódigo do Merge Sort que tipicamente rege as implementações em Java e C. Em Python podemos utilizar os recursos da linguagem para pegar os índices que separam o arranjo esquerdo e o arranjo direito quando temos o meio calculado.

\begin{lstlisting}[label=merge,caption= Merge]
def merge(esq, dir):
    i, j = 0, 0
    ordenados = list()

    while i < len(esq) and j < len(dir):
        if esq[i] <= dir[j]:
            ordenados.append(esq[i])
            i += 1
        else:
            ordenados.append(dir[j])
            j += 1
    
    if i < len(esq):
        ordenados += esq[i:]
    
    elif j < len(dir):
        ordenados += dir[j:]
    
    return ordenados
\end{lstlisting}

O merge funciona verificando o arranjo esquerdo e o direito para identificar quais valores são menores (do arranjo esquerdo ou do direito) e até quando. Os valores são armazenados em uma lista quer será retornada ao no método $merge\_sort()$. Também são verificados se houveram valores que não foram inseridos na comparação, ou seja, os valores do arranjo esquerdo ou direito que eram maiores que o maior valor do outro subarranjo.
    
Para analisar a complexidade do merge sort precisamos notar que ele realiza duas chamadas recursivas para $n/2$ e uma chamada ao merge para cada $n$. Considerando a operação de interesse a comparação $esq[i] <= dir[j]$ do laço da função merge, teremos um custo para essa função de $n-1$ no pior caso e $n/2$ no melhor caso. A equação de recorrência para o pior caso é:

$T(n) = \begin{cases} 
	0, \ n \leq 1\\
	2T(n/2) + n-1, \ n > 1
\end{cases}$ \\

E calculando essa equação de divisão e conquista pelo Teorema Mestre (cláusula 2) descobrimos que a complexidade assintótica do merge sort para todos os casos  $\in \Theta(n*log n)$. Vale lembrar que para a generalização é necessário calcular a equação de recorrência para o melhor caso também, que terá a mesma complexidade que o pior caso.

É impotante observar que ainda que a complexidade do Merge Sort seja menor que o dos outros estudados até então, esse algoritmo tem um custo de criar novos arranjos para manipular os elementos durante a execução do método.

\section{Quick Sort}
O quick sort segue a mesma hipótese de indução forte que o merge sorte (sabemos ordenar um novo elemento em qualquer arranjo até então ordenado). O método consiste em dividir o arranjo em uma partição central onde todos os elementos à esquerda dela são menores que esse valor e à direita são maiores. O processo é repetido até termos apenas um elemento para procurar a partição, e então é realizada a volta das chamadas recursivas. O método de ordenação é desenvolvido utilizando duas funções:

\begin{lstlisting}[label=quick_sort,caption= Quick Sort]
def quick_sort(A, ini, fim):
    if ini < fim:
        p = particao(A, ini, fim)
        quick_sort(A, ini, p-1)
        quick_sort(A, p+1, fim)
    return A
\end{lstlisting}

Como pode ser visto, há um processo de divisão custoso (chamada à função partição) e um processo de conquista pela volta às chamadas recursivas. Não há um custo para junção, como visto no merge sort. A função em questão realiza as chamadas recursivas para o mesmo arranjo à esquerda da partição e à direita dela, atualizando diretamente no arranjo passado na entrada. A função $particao()$ que realiza a magia é mostrada na sequência.

Para identificar o elemento que divide o arranjo entre maiores e menores que um valor é chamada a função de partição e retornado o tal elemento da divisão, chamado de pivô. Com um arranjo qualquer nós começamos definindo os índices \textbf{válidos} que limitarão o inicio e fim do processo de comparação no arranjo (diferente do merge sort, os índice de fim do arranjo deve ser uma posição valida, e não o tamanho). 

Enquanto os índices forem diferentes haverá um processo de comparar os elementos à esquerda do arranjo com o pivô para  verificar se são menores que ele, e a mesma coisa para os elmeentos à direita serem maiores. Caso à esquerda seja realmente menor, está tudo certo por enquanto e vamos para a próxima posição. Caso o elemento à esquerda seja maior que o pivô (isto é, deve ir para a direita do arranjo) devemos verificar se o elemento à direita é maior ou menor que o pivô. Se o elemento à direita (j) for maior, ele está na posição correta e passamos a comparação do elemento à esquerda (i) para o elemento à esquerda do mais à direita (j-1). Se o da  esquerda for maior que o pivô e o à direita for menor, trocamos eles de lugar e atualizamos seus índices. Ao fim é criado atualizado o pivô como o elemento que ficou ao meio da "ordenação" atual. Esse algoritmo de partição também é chamado de partição de Hoare.

\begin{lstlisting}[label=quick_sort,caption= Partição]
def particao(A, ini, fim):
    pivo = A[fim]
    i = ini
    j = fim - 1

    while i <= j:
        if A[i] <= pivo:
            i += 1
        elif A[j] > pivo:
            j -= 1
        else:
            aux = A[i]
            A[i] = A[j]
            A[j] = aux
            i += 1
            j -= 1
    
    A[fim] = A[i]
    A[i] = pivo
    return i
\end{lstlisting}

Essa implementação da partição tem como melhor caso para o número de comparações entre o pivô e um elmento do arranjo duas possibilidades. A primeira é quando o pivô é o maior elemento do subarranjo (realiza apenas a primeira comparação do método partição), e a segunda é quando em uma passada do laço realiza as duas comparações e cai no $else$, isto é, quando deve haver uma troca e, portanto, avança tanto do lado esquerdo quanto do lado direito do subarranjo (pula uma das comparações que poderiam ser feitas). Para o primeiro melhor caso serão feitas $n-1$ comparações, e para o segundo caso serão feitas também $2(n-1)/2 = n-1$ comparações.

O pior caso é quando o primeiro if é verdadeiro e o segundo é falso, ou seja, quando o pivô é menor que todos os elementos à esquerda e menor que todos à direita (logo, o menor elemento do subarranjo). Realiza-se nesse caso $2n-2$ comparações. Tanto no melhor quanto no pior caso, $T(n) \in \Theta(n)$

O quick sort como um todo tem uma equação de recorrência para o melhor caso definida como:

$T(n) = \begin{cases} 
	0, \ n \leq 1\\
	2T(n/2) + n-1, \ n > 1
\end{cases}$ \\
Com $T(n) \in \Theta(n* \log n)$ \\

E como pior caso:

$T(n) = \begin{cases} 
	0, \ n \leq 1\\
	T(n-1) + 2n-2, \ n > 1
\end{cases}$ \\
Com $T(n) \in \Theta(n^2)$\\

Uma prova mais longa pode provar que o caso médio do quick sort $\in \Theta(n* log n)$

\section{Heap Sort}
Heaps em computação são utilizados como um espaço de memória para armazenar valores e como uma estrutura de dados para uma regra particular. No Heaps sort, essa estrutura é auxiliar ao método. A ordenação desse método utiliza indução fraca (a mesma do selection sort), mas a estrutura Heap utiliza indução forte. A organização do Heap permite otimizar a ordenação no método, contrariando a ordenação por seleção. O Heap vai simular uma árvore binária completa em um arranjo.

Cada posição do arranjo representa um nó, todos os níveis são completos, com exeção das folhas. Para encontrar as relações de um nó na posição $i$ segue-se o método:

$PAI: \lfloor(i-1)/2 \rfloor $

$ESQUERDA: 2*i + 1$

$DIREITA: 2*i + 2 $

Aqui utilizaremos o conceito de Heap máximo, onde um nó pai é maior ou igual que seus filhos. Em um heap de altura $h$ o número de nós varia de $2^h \leq n \leq 2^{h+1}$, sendo a altura definida como a distância até as folhas para um nó, portanto a altura de um heap e n nós é $\lfloor \log2 n \rfloor$.

A implementação do Heap será baseada em duas funções. A primeira é uma função recursiva que verifica se um nó pai é maior que seus filhos à esquerda e à direita. Caso seja, trocam-se de valores e se faz a verificação novamente (por recursão para o novo nó pai). 

\begin{lstlisting}[label=refazer_heap,caption= Manutenção do Heap]
def refazer_heap_max(A, i_pai, n_heap):
    esq = 2*i_pai + 1 
    dir = 2*i_pai + 2

    if esq < n_heap and A[esq] > A[i_pai]:
        maior = esq
    else:
        maior = i_pai
    
    if dir < n_heap and A[dir] > A[maior]:
        maior = dir
    
    if maior != i_pai:
        aux = A[maior]
        A[maior] = A[i_pai]
        A[i_pai] = aux
        refazer_heap_max(A, maior, n_heap)
\end{lstlisting}

Essa função será passada para a construção do Heap (máximo). O primeiro nó pai é o que está no meio do arranjo, e a partir daí é chamado o método anterior para rearranjar o heap, ou seja, deixar um nó pai maior que seus filhos. Esse processo vai ser realizado até o primeiro elemento do arranjo ($n/2$ iterações). 

\begin{lstlisting}[label=construir_heap,caption= Construção do Heap]
def construir_heap_max(A):
    n_heap = len(A)
    i = (n_heap // 2) - 1
    while i >= 0:
        refazer_heap_max(A, i, n_heap)
        i -= 1
\end{lstlisting}

Com o Heap implementado, podemos utilizar o Heap Sort para ordenar os valores de uma lista / arranjo. O método se baseia no Selection Sort, mas a estrutura do heap influencia em uma redução na quantidade de iterações necessárias, logo, na complexidade assintótica.

\begin{lstlisting}[label=heap_sort,caption= Heap Sort]
def heap_sort(A):
    construir_heap_max(A)
    n_heap = len(A)
    i = len(A) - 1
    while i > 0:
        aux = A[0]
        A[0] = A[i]
        A[i] = aux
        n_heap -= 1
        i -= 1
        refazer_heap_max(A, 0, n_heap)
\end{lstlisting}

A operação de interesse para a análise da complexidade assintótica do Heap Sort é a comparação entre elementos do arranjo, passo esse que acontece no método $\texttt{refazer\_heap\_max()}$. As comparações $\texttt{A[esq]>A[i\_pai]}$ e $\texttt{A[dir]>A[maior]}$ acontecem, no pior caso, em  $2/3$ dos elementos, apenas uma vez. O pior caso geral a recursão acontece para metade dos elementos, fazendo duas verificações por vez.\\

$T(n) = \begin{cases} 
	0, \ n \leq 1\\
	T(n/2) + 2, \ n > 1
\end{cases} \in \mathcal{O}(\log n)$ \\

A função $\texttt{construir\_heap\_max()}$ pode ser reformulada como uma função recursiva para fazer uma equação de recorrência. Neste caso seria feita uma chamada à esquerda e outra à direita, ambas para metade dos elementos. O custo seria acrescido pelas comparações do método de manutenção do heap.

$T(n) = \begin{cases} 
	0, \ n \leq 1\\
	2T(n/2) + \mathcal{O}(\log n), \ n > 1
\end{cases} \\ T(n) \in \Theta(n)$ \\

O Heap Sort tem o custo $\Theta(n)$ para criar o Heap e o custo $\mathcal{O}(\log n)$ para realizar a manutenção do Heap. Assim, o número de operações $n\_op$ pode ser identificado como:

$n\_op \in \Theta(n) + \mathcal{O}(\log n)$, e como $\Theta(n)$ cresce com $\mathcal{O}(n)$:

$n\_op \in \mathcal{O}(n) + \mathcal{O}(\log n)$

$n\_op \in \mathcal{O}(n \log n)$


\section{Couting Sort}
A ordenação por comparações (algoritmos vistos até aqui) não conseguem ordenar um arranjo em tempo linear. Para tal é importante buscar adaptações ao método de pensar a ordenação de arranjos e assim buscar algum método de complexidade $\mathcal{O}(n)$. Uma das possibilidades é por meio da ordenação por contagem.

O Couting Sort deve receber, além do arranjo, um parâmetro adicional $k \in \mathcal{O}(n)$, isto é, os $n$ elementos de um arranjo estão no intervalo $0 \leq < k$. É possível realizar modificações para o Couting Sort aceitar valores negativos (como referenciando eles ao fim do arranjo), mas esse método não é focado para esses casos. A ordenação por contagem consegue ter complexidade linear, mas a restrição de que os elementos estejam no intervalo entre 0 e n tipicamente deixa o método para contextos bem específicos, mas quando entram neste escopo acabam se tornando interessantes. 

Outro problema desse algoritmo é que se torna necessário alocar em memória espaço para todos elementos possíveis de se entrar no arranjo ordenado (entre 0 e k), mesmo que não sejam utilizados.

Um ponto positivo da implementação é que ela é estável, isto é, elementos de mesmo valor que aparecem em posições diferentes no arranjo original são ordenados na mesma sequência.

\begin{lstlisting}[label=couting_sort,caption= Couting Sort]
def couting_sort(A, k):
    n = len(A)
    ordenado = [None] * n
    quantidade = [0] * k

    for j in range(n): 
        quantidade[A[j]] += 1
    for i in range(1, k): 
        quantidade[i] += quantidade[i-1]
    for j in reversed(range(0, n)):
        ordenado[quantidade[A[j]]-1] = A[j]
        quantidade[A[j]] -= 1

    return ordenado
\end{lstlisting}

A ordenação por contagem não realiza nenhuma comparação, logo, a operação de interesse é a de atribuiçãod de valores. Analisando o algoritmo podemos perceber que a quantidade de operações de interessse $\in \Theta(n+k)$ e pela restrição $k \in \mathcal{O}(n)$, que o método ideal utiliza, o algoritmo Couting Sort tem crescimento assintótico $\mathcal{O}(n)$ .

\section{Radix Sort}

O último método de ordenação à ser analisado é o Radix Sort, ou ordenação por raíz. Esse método de ordenação linear baseia-se na extração de cada parte de um valor (normalmente uma string) a partir do dígito menos significativo (de trás pra frente). Para cada um dos dígitos será chamado um ordenador com complexidade $\mathcal{O}(n)$, visto que não faz sentido utilizar um ordenador de custo não linear em um ordenador que visa custo linear. Aqui exemplificaremos o uso do Radix Sort pela ordenação de strings de tamanho fixo com auxílio dos valores unicode de seus dítigos (limitando o úniverso aos primeiros 256 valores unicode). O método usará como ordenador uma adaptação do couting sort que ordena strings.

\begin{lstlisting}[label=radix_sort,caption= Radix Sort para strings de tamanho fixo]
def radix_sort(A, n_digitos):
    n_unicode = 256
    for i in reversed(range(n_digitos)):
        A = couting_sort_str(A, i, n_unicode)
        print(A)
    return 0
\end{lstlisting}

O método recebe o vetor de strings, com cada entrada de tamanho $n\_digitos$. Para cada dígito (do menos significativo ao mais significativo) é chamado o couting sort para ordenar a string pelos valores unicode.

\begin{lstlisting}[label=couting_sort_str, caption= Couting Sort para valores unicode de caracteres]
def couting_sort_str(A, indice, k):
 n = len(A)
 ordenado = [''] * n
 quantidade = [0] * k

 for j in range(n): 
  quantidade[ord(A[j][indice])] += 1
 for i in range(1, k): 
  quantidade[i] += quantidade[i-1]
 for j in reversed(range(0, n)):
  ordenado[quantidade[ord(A[j][indice])]-1] = A[j]
  quantidade[ord(A[j][indice])] -= 1
 return ordenado
\end{lstlisting}

O comando \texttt{ord} em python retorna o valor unicode de um caractere. O método acima permite executar o couting sort sobre caracteres de uma string e retornar o arranjo ordenado com base em um dos dígitos das strings.

A complexidade do Radix Sort, portanto, é dependente da complexidade do ordenador secundário chamado. Utilizando um ordenador de complexidade $\Theta(n)$ como o Couting Sort (considerando, obviamente, que suas restrições para tamanho de $k$ tenham sido obedecidas), a complexidade do Radix Sort será $\Theta(d*\Theta(n))$, onde $d$ é a quantidade de divisões feitas no radix sort (no exemplo dado, seria a quantidade de dígitos de uma string). Se, e somente se $d$ for um valor independente de $n$, poderemos considerar $d \in \Theta(1)$, e assim o Radix Sort terá complexidade $\Theta(n)$.




\end{document}